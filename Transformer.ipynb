{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Instead of the U-Net, we will build a Transformer Encoder model. The key difference from first Transformer (which predicted BP) is how we handle the output.\n",
        "\n",
        "BP Model (Sequence-to-Value): Input (250, 2) -> Transformer Blocks -> GlobalAveragePooling -> Dense(2)\n",
        "\n",
        "ECG Model (Sequence-to-Sequence): Input (256, 1) -> Transformer Blocks -> TimeDistributed(Dense(1))\n",
        "\n",
        "We will remove the GlobalAveragePooling1D layer. This way, the model outputs a full sequence, not just a single value. We'll use the data processing functions from the U-Net code, as they are already set up for this task.\n",
        "\n",
        "Here is the complete code to train a Transformer for PPG-to-ECG synthesis."
      ],
      "metadata": {
        "id": "VPgrkeR5bDAh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3CnSzc0a_fF"
      },
      "outputs": [],
      "source": [
        "# === 1. Install and Import Libraries ===\n",
        "\n",
        "# Install xlrd if not already present\n",
        "!pip install -q xlrd\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "import shutil\n",
        "\n",
        "# Sklearn for metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# TensorFlow/Keras for Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Add\n",
        "from tensorflow.keras.layers import LayerNormalization, Embedding\n",
        "from tensorflow.keras.layers import MultiHeadAttention, TimeDistributed, GlobalAveragePooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# === 2. Mount Drive & Define Data Functions ===\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def unzip_data(zip_path, extract_folder):\n",
        "    \"\"\"Unzips a file and returns a list of all .csv files inside.\"\"\"\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Error: {zip_path} not found. Check your Google Drive path.\")\n",
        "        return []\n",
        "    os.makedirs(extract_folder, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_folder)\n",
        "    csv_files = glob.glob(os.path.join(extract_folder, '**/*.csv'), recursive=True)\n",
        "    print(f\"Extracted {len(csv_files)} files from {zip_path}\")\n",
        "    return csv_files\n",
        "\n",
        "def create_sequences_ppg_to_ecg(df, seq_length=256, step=128):\n",
        "    \"\"\"\n",
        "    Creates overlapping sequences for PPG-to-ECG translation.\n",
        "    Input (X) is PPG signal.\n",
        "    Output (y) is ECG signal.\n",
        "    \"\"\"\n",
        "    ecg = df['ECG'].values\n",
        "    ppg = df['PPG'].values\n",
        "\n",
        "    # Normalize signals individually\n",
        "    ecg_mean, ecg_std = np.mean(ecg), np.std(ecg) + 1e-6\n",
        "    ppg_mean, ppg_std = np.mean(ppg), np.std(ppg) + 1e-6\n",
        "\n",
        "    ecg = (ecg - ecg_mean) / ecg_std\n",
        "    ppg = (ppg - ppg_mean) / ppg_std\n",
        "\n",
        "    X_seq = []\n",
        "    y_seq = []\n",
        "\n",
        "    for i in range(0, len(df) - seq_length, step):\n",
        "        end_idx = i + seq_length\n",
        "\n",
        "        X_window = ppg[i:end_idx]\n",
        "        y_window = ecg[i:end_idx]\n",
        "\n",
        "        if np.std(X_window) > 0.1 and np.std(y_window) > 0.1:\n",
        "            X_seq.append(X_window)\n",
        "            y_seq.append(y_window)\n",
        "\n",
        "    # Add a \"channels\" dimension for Conv1D/Transformer\n",
        "    return np.expand_dims(np.array(X_seq), -1), np.expand_dims(np.array(y_seq), -1)\n",
        "\n",
        "def load_and_process(zip_path, extract_folder, seq_length=256, debug_limit=None):\n",
        "    \"\"\"Main function to load zips and process all files for sequence models.\"\"\"\n",
        "    file_list = unzip_data(zip_path, extract_folder)\n",
        "    if debug_limit is not None:\n",
        "        file_list = file_list[:debug_limit]\n",
        "        print(f\"--- DEBUG MODE: Processing only {len(file_list)} files. ---\")\n",
        "\n",
        "    if not file_list: return np.array([]), np.array([])\n",
        "    all_X, all_y = [], []\n",
        "\n",
        "    for f in tqdm(file_list, desc=f\"Processing {zip_path}\"):\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read {f}: {e}\")\n",
        "            continue\n",
        "        if not all(col in df.columns for col in ['t_sec', 'ECG', 'PPG', 'ABP']):\n",
        "            print(f\"Skipping {f}: missing required columns.\")\n",
        "            continue\n",
        "\n",
        "        X, y = create_sequences_ppg_to_ecg(df, seq_length=seq_length)\n",
        "        if X.shape[0] > 0:\n",
        "            all_X.append(X)\n",
        "            all_y.append(y)\n",
        "\n",
        "    if not all_X:\n",
        "        print(f\"No valid data found in {zip_path} for sequence mode.\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    all_X = np.concatenate(all_X, axis=0)\n",
        "    all_y = np.concatenate(all_y, axis=0)\n",
        "    print(f\"Finished processing {zip_path}. Found {all_X.shape[0]} samples.\")\n",
        "    return all_X, all_y\n",
        "\n",
        "# === 3. Transformer Model Definition ===\n",
        "\n",
        "def transformer_encoder_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    \"\"\"Creates a single Transformer encoder block.\"\"\"\n",
        "    # Attention and Normalization\n",
        "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    res = Add()([x, inputs]) # Residual connection\n",
        "\n",
        "    # Feed-Forward Network and Normalization\n",
        "    x = LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = Dense(inputs.shape[-1])(x)\n",
        "    return Add()([x, res]) # Second residual connection\n",
        "\n",
        "def build_transformer_seq2seq_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    embed_dim, # We define embedding dim directly\n",
        "    dropout=0,\n",
        "):\n",
        "    \"\"\"Builds a Transformer-based model for sequence-to-sequence regression.\"\"\"\n",
        "    inputs = Input(shape=input_shape) # e.g., (256, 1)\n",
        "    x = inputs\n",
        "\n",
        "    # --- 1. Create an \"Embedding\" for the time-series data ---\n",
        "    # Project the 1 feature (PPG) into a higher-dimensional space (embed_dim)\n",
        "    x = Dense(embed_dim)(x)\n",
        "\n",
        "    # --- 2. Positional Encoding ---\n",
        "    # We add a simple learned positional embedding.\n",
        "    positions = tf.range(start=0, limit=input_shape[0], delta=1)\n",
        "    position_embedding = Embedding(input_dim=input_shape[0], output_dim=embed_dim)(positions)\n",
        "    x = x + position_embedding\n",
        "\n",
        "    # --- 3. Create Transformer Blocks ---\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder_block(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    # --- 4. Final Head for Sequence Output ---\n",
        "    # We DON'T pool. Instead, we apply a Dense layer to *every time step*.\n",
        "    # This projects the embed_dim back down to 1 (our ECG signal).\n",
        "    outputs = TimeDistributed(Dense(1, activation=\"linear\"))(x)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# === 4. Transformer Model Training and Evaluation ===\n",
        "\n",
        "print(\"\\n--- Starting Transformer PPG-to-ECG Model ---\")\n",
        "\n",
        "# 1. Define Model Parameters\n",
        "SEQ_LENGTH = 256  # Power of 2 is good for these models\n",
        "STEP = 128\n",
        "NUM_FEATURES = 1  # Input is just PPG\n",
        "NUM_OUTPUTS = 1   # Output is just ECG\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "\n",
        "# --- Transformer-specific Hyperparameters ---\n",
        "EMBED_DIM = 64    # Dimension to project PPG signal into\n",
        "HEAD_SIZE = 64    # Dimension of each attention head\n",
        "NUM_HEADS = 4\n",
        "FF_DIM = 128      # Hidden layer size in Feed-Forward network\n",
        "NUM_BLOCKS = 3    # Number of Transformer blocks\n",
        "DROPOUT = 0.1\n",
        "# ---------------------------------------------\n",
        "\n",
        "# --- Define Paths ---\n",
        "# !!! EDIT THESE PATHS !!!\n",
        "train_zip_path = '/content/drive/MyDrive/11785FinalData/train.zip'\n",
        "val_zip_path = '/content/drive/MyDrive/11785FinalData/val.zip'\n",
        "test_zip_path = '/content/drive/MyDrive/11785FinalData/test.zip'\n",
        "\n",
        "# 2. Load and process data\n",
        "X_train_seq, y_train_seq = load_and_process(train_zip_path, 'data/train', seq_length=SEQ_LENGTH)\n",
        "X_val_seq, y_val_seq = load_and_process(val_zip_path, 'data/val', seq_length=SEQ_LENGTH)\n",
        "X_test_seq, y_test_seq = load_and_process(test_zip_path, 'data/test', seq_length=SEQ_LENGTH)\n",
        "\n",
        "if X_train_seq.shape[0] == 0:\n",
        "    print(\"No training data found for sequence-based model. Aborting.\")\n",
        "else:\n",
        "    print(f\"Training data shape: {X_train_seq.shape}\")\n",
        "    print(f\"Training labels shape: {y_train_seq.shape}\")\n",
        "\n",
        "    # 3. Build and compile the Transformer model\n",
        "    input_shape = (SEQ_LENGTH, NUM_FEATURES)\n",
        "\n",
        "    model = build_transformer_seq2seq_model(\n",
        "        input_shape,\n",
        "        head_size=HEAD_SIZE,\n",
        "        num_heads=NUM_HEADS,\n",
        "        ff_dim=FF_DIM,\n",
        "        num_transformer_blocks=NUM_BLOCKS,\n",
        "        embed_dim=EMBED_DIM,\n",
        "        dropout=DROPOUT,\n",
        "    )\n",
        "\n",
        "    # Use 'mean_squared_error' as the loss for signal regression\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "    model.summary()\n",
        "\n",
        "    # 4. Train Model\n",
        "    print(\"\\nTraining Transformer model...\")\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 5. Evaluate on Test Set\n",
        "    print(\"\\nEvaluating Transformer on test set...\")\n",
        "    # This will return [test_loss, test_mae]\n",
        "    results = model.evaluate(X_test_seq, y_test_seq, batch_size=BATCH_SIZE)\n",
        "    test_loss = results[0]\n",
        "    test_mae = results[1]\n",
        "\n",
        "    # 6. Report Results\n",
        "    print(\"\\n--- Transformer Model Test Results ---\")\n",
        "    print(f\"Test Set MSE (Loss): {test_loss:.4f}\")\n",
        "    print(f\"Test Set MAE:        {test_mae:.4f}\")\n",
        "    print(\"--------------------------------------\")\n",
        "\n",
        "\n",
        "# === 5. Save a Trained Model to Your Drive ===\n",
        "\n",
        "# First, create a path to a folder in your Google Drive\n",
        "save_folder = '/content/drive/My Drive/MyProject'\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "# Define the full path to save your model file\n",
        "model_save_path = os.path.join(save_folder, 'transformer_ppg_to_ecg_model.keras')\n",
        "\n",
        "# Save the model\n",
        "try:\n",
        "    model.save(model_save_path)\n",
        "    print(f\"Model successfully saved to: {model_save_path}\")\n",
        "except NameError:\n",
        "    print(\"Could not save model. Make sure you have trained the model and it is in a variable named 'model'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving: {e}\")"
      ]
    }
  ]
}