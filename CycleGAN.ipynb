{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This block imports all standard libraries. We also install and import tensorflow-addons, which provides InstanceNormalization, a layer that is critical for high-quality GAN results."
      ],
      "metadata": {
        "id": "bRERw-Vxg21M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA7UBeNdg1dc"
      },
      "outputs": [],
      "source": [
        "# === 1. Install and Import Libraries ===\n",
        "\n",
        "# Install tensorflow-addons for InstanceNormalization\n",
        "!pip install -q tensorflow-addons\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "# Sklearn for metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, LeakyReLU\n",
        "from tensorflow.keras.layers import Conv1D, Conv1DTranspose\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# TensorFlow Addons\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section contains the exact same data loading and processing functions as the previous models. Their job is to read the CSVs from the .zip files and convert them into NumPy arrays of paired (PPG_window, ECG_window) samples.\n",
        "\n",
        "We will not change these, but in the next section, we will feed their output into a tf.data pipeline, which is necessary for training a GAN."
      ],
      "metadata": {
        "id": "mnz8ZdH0g6K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 2. Data Loading Functions ===\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def unzip_data(zip_path, extract_folder):\n",
        "    \"\"\"Unzips a file and returns a list of all .csv files inside.\"\"\"\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Error: {zip_path} not found. Check your Google Drive path.\")\n",
        "        return []\n",
        "    os.makedirs(extract_folder, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_folder)\n",
        "    csv_files = glob.glob(os.path.join(extract_folder, '**/*.csv'), recursive=True)\n",
        "    print(f\"Extracted {len(csv_files)} files from {zip_path}\")\n",
        "    return csv_files\n",
        "\n",
        "def create_sequences_ppg_to_ecg(df, seq_length=256, step=128):\n",
        "    \"\"\"\n",
        "    Creates overlapping sequences for PPG-to-ECG translation.\n",
        "    Input (X) is PPG signal.\n",
        "    Output (y) is ECG signal.\n",
        "    \"\"\"\n",
        "    ecg = df['ECG'].values\n",
        "    ppg = df['PPG'].values\n",
        "\n",
        "    # Normalize signals individually\n",
        "    ecg = (ecg - np.mean(ecg)) / (np.std(ecg) + 1e-6)\n",
        "    ppg = (ppg - np.mean(ppg)) / (np.std(ppg) + 1e-6)\n",
        "\n",
        "    X_seq = []\n",
        "    y_seq = []\n",
        "\n",
        "    for i in range(0, len(df) - seq_length, step):\n",
        "        end_idx = i + seq_length\n",
        "\n",
        "        X_window = ppg[i:end_idx]\n",
        "        y_window = ecg[i:end_idx]\n",
        "\n",
        "        if np.std(X_window) > 0.1 and np.std(y_window) > 0.1:\n",
        "            X_seq.append(X_window)\n",
        "            y_seq.append(y_window)\n",
        "\n",
        "    # Add a \"channels\" dimension\n",
        "    return np.expand_dims(np.array(X_seq), -1), np.expand_dims(np.array(y_seq), -1)\n",
        "\n",
        "def load_and_process(zip_path, extract_folder, seq_length=256, debug_limit=None):\n",
        "    \"\"\"Main function to load zips and process all files for sequence models.\"\"\"\n",
        "    file_list = unzip_data(zip_path, extract_folder)\n",
        "    if debug_limit is not None:\n",
        "        file_list = file_list[:debug_limit]\n",
        "        print(f\"--- DEBUG MODE: Processing only {len(file_list)} files. ---\")\n",
        "\n",
        "    if not file_list: return np.array([]), np.array([])\n",
        "    all_X, all_y = [], []\n",
        "\n",
        "    for f in tqdm(file_list, desc=f\"Processing {zip_path}\"):\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read {f}: {e}\")\n",
        "            continue\n",
        "        if not all(col in df.columns for col in ['t_sec', 'ECG', 'PPG', 'ABP']):\n",
        "            print(f\"Skipping {f}: missing required columns.\")\n",
        "            continue\n",
        "\n",
        "        X, y = create_sequences_ppg_to_ecg(df, seq_length=seq_length)\n",
        "        if X.shape[0] > 0:\n",
        "            all_X.append(X)\n",
        "            all_y.append(y)\n",
        "\n",
        "    if not all_X:\n",
        "        print(f\"No valid data found in {zip_path} for sequence mode.\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    all_X = np.concatenate(all_X, axis=0)\n",
        "    all_y = np.concatenate(all_y, axis=0)\n",
        "    print(f\"Finished processing {zip_path}. Found {all_X.shape[0]} samples.\")\n",
        "    return all_X, all_y"
      ],
      "metadata": {
        "id": "Dm9h59_rg8W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a new, crucial step. GANs must be trained with a tf.data pipeline for performance. This code:\n",
        "\n",
        "Sets your file paths (you must edit these!).\n",
        "\n",
        "Loads the data into NumPy arrays using the functions from Section 2.\n",
        "\n",
        "Converts these NumPy arrays into tf.data.Dataset objects.\n",
        "\n",
        "Applies .shuffle(), .batch(), and .prefetch() to create a high-performance, GPU-ready data pipeline."
      ],
      "metadata": {
        "id": "5PEIYmxIg-HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 3. Configuration & tf.data Pipeline ===\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "SEQ_LENGTH = 256\n",
        "STEP = 128\n",
        "BUFFER_SIZE = 1000  # For shuffling\n",
        "BATCH_SIZE = 1      # CycleGANs are often trained with batch_size=1\n",
        "EPOCHS = 20\n",
        "\n",
        "# --- Define Paths ---\n",
        "# !!! EDIT THESE PATHS !!!\n",
        "train_zip_path = '/content/drive/MyDrive/11785FinalData/train.zip'\n",
        "val_zip_path = '/content/drive/MyDrive/11785FinalData/val.zip'\n",
        "test_zip_path = '/content/drive/MyDrive/11785FinalData/test.zip'\n",
        "\n",
        "# 1. Load data into NumPy arrays\n",
        "print(\"--- Loading Training Data ---\")\n",
        "X_train_ppg, y_train_ecg = load_and_process(train_zip_path, 'data/train', seq_length=SEQ_LENGTH)\n",
        "print(\"--- Loading Test Data ---\")\n",
        "X_test_ppg, y_test_ecg = load_and_process(test_zip_path, 'data/test', seq_length=SEQ_LENGTH)\n",
        "\n",
        "if X_train_ppg.shape[0] > 0:\n",
        "    # 2. Create tf.data.Dataset\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_ppg, y_train_ecg))\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test_ppg, y_test_ecg))\n",
        "\n",
        "    # 3. Optimize pipeline\n",
        "    train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "    test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    print(\"\\n--- tf.data pipeline created ---\")\n",
        "    print(f\"Train Dataset: {train_dataset}\")\n",
        "    print(f\"Test Dataset: {test_dataset}\")\n",
        "else:\n",
        "    print(\"--- ERROR: No training data was loaded. Aborting. ---\")"
      ],
      "metadata": {
        "id": "0r6DSJX6hAm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define the four models required for a CycleGAN:\n",
        "\n",
        "Generator (U-Net): A 1D U-Net architecture, similar to before. It uses InstanceNormalization instead of BatchNormalization. We will use the same build_generator function for both G: PPG->ECG and F: ECG->PPG.\n",
        "\n",
        "Discriminator (PatchGAN): A 1D CNN that acts as a classifier. It doesn't output a single \"real/fake\" (0/1). Instead, it outputs a sequence of predictions (e.g., (30, 1)). This \"PatchGAN\" approach is more stable. We will use build_discriminator for both D_X (PPG) and D_Y (ECG)."
      ],
      "metadata": {
        "id": "g7gds7zmhFIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 4. Model Architecture ===\n",
        "\n",
        "# Use InstanceNormalization\n",
        "InstanceNorm = tfa.layers.InstanceNormalization\n",
        "OUTPUT_CHANNELS = 1\n",
        "INPUT_SHAPE = (SEQ_LENGTH, 1)\n",
        "\n",
        "# Weight initializer for GANs\n",
        "init = RandomNormal(stddev=0.02)\n",
        "\n",
        "def downsample(filters, size, apply_instancenorm=True):\n",
        "    \"\"\"A 1D downsampling block (Conv -> InstanceNorm -> LeakyReLU)\"\"\"\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(Conv1D(filters, size, strides=2, padding='same', kernel_initializer=init, use_bias=False))\n",
        "    if apply_instancenorm:\n",
        "        result.add(InstanceNorm())\n",
        "    result.add(LeakyReLU())\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    \"\"\"A 1D upsampling block (ConvTranspose -> InstanceNorm -> (Dropout) -> ReLU)\"\"\"\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(Conv1DTranspose(filters, size, strides=2, padding='same', kernel_initializer=init, use_bias=False))\n",
        "    result.add(InstanceNorm())\n",
        "    if apply_dropout:\n",
        "        result.add(Dropout(0.5))\n",
        "    result.add(LeakyReLU())\n",
        "    return result\n",
        "\n",
        "def build_generator(input_shape=INPUT_SHAPE, name=\"generator\"):\n",
        "    \"\"\"Builds the 1D U-Net Generator\"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Downsampling path\n",
        "    d1 = downsample(64, 4, apply_instancenorm=False)(inputs) # (128, 64)\n",
        "    d2 = downsample(128, 4)(d1) # (64, 128)\n",
        "    d3 = downsample(256, 4)(d2) # (32, 256)\n",
        "    d4 = downsample(512, 4)(d3) # (16, 512)\n",
        "\n",
        "    # Bottleneck\n",
        "    bottleneck = Conv1D(512, 4, strides=2, padding='same', kernel_initializer=init)(d4) # (8, 512)\n",
        "    bottleneck = LeakyReLU()(bottleneck)\n",
        "\n",
        "    # Upsampling path\n",
        "    u4 = upsample(512, 4)(bottleneck) # (16, 512)\n",
        "    u4 = Concatenate()([u4, d4]) # Skip connection\n",
        "\n",
        "    u3 = upsample(256, 4)(u4) # (32, 256)\n",
        "    u3 = Concatenate()([u3, d3]) # Skip connection\n",
        "\n",
        "    u2 = upsample(128, 4)(u3) # (64, 128)\n",
        "    u2 = Concatenate()([u2, d2]) # Skip connection\n",
        "\n",
        "    u1 = upsample(64, 4)(u2) # (128, 64)\n",
        "    u1 = Concatenate()([u1, d1]) # Skip connection\n",
        "\n",
        "    # Final layer\n",
        "    last = Conv1DTranspose(OUTPUT_CHANNELS, 4, strides=2, padding='same', kernel_initializer=init, activation='tanh')(u1) # (256, 1)\n",
        "\n",
        "    return Model(inputs, last, name=name)\n",
        "\n",
        "def build_discriminator(input_shape=INPUT_SHAPE, name=\"discriminator\"):\n",
        "    \"\"\"Builds the 1D PatchGAN Discriminator\"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    d1 = Conv1D(64, 4, strides=2, padding='same', kernel_initializer=init)(inputs) # (128, 64)\n",
        "    d1 = LeakyReLU(0.2)(d1)\n",
        "\n",
        "    d2 = Conv1D(128, 4, strides=2, padding='same', kernel_initializer=init, use_bias=False)(d1) # (64, 128)\n",
        "    d2 = InstanceNorm()(d2)\n",
        "    d2 = LeakyReLU(0.2)(d2)\n",
        "\n",
        "    d3 = Conv1D(256, 4, strides=2, padding='same', kernel_initializer=init, use_bias=False)(d2) # (32, 256)\n",
        "    d3 = InstanceNorm()(d3)\n",
        "    d3 = LeakyReLU(0.2)(d3)\n",
        "\n",
        "    # Patch output\n",
        "    patch_out = Conv1D(1, 4, strides=1, padding='same', kernel_initializer=init)(d3) # (32, 1)\n",
        "\n",
        "    return Model(inputs, patch_out, name=name)"
      ],
      "metadata": {
        "id": "qUZWPYHRhGeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the most complex part of the CycleGAN. We define:\n",
        "\n",
        "Four Optimizers: One for each of our four networks.\n",
        "\n",
        "Loss Objects: BinaryCrossentropy for the adversarial (GAN) loss and MeanAbsoluteError (L1) for the cycle and identity losses.\n",
        "\n",
        "Loss Functions:\n",
        "\n",
        "discriminator_loss: Tries to make the discriminator output 1 for real signals and 0 for fake signals.\n",
        "\n",
        "generator_loss: Tries to make the discriminator output 1 for the generator's fake signals.\n",
        "\n",
        "calc_cycle_loss: The L1 error between a real signal and the reconstructed signal.\n",
        "\n",
        "calc_identity_loss: (Optional but good) The L1 error when a generator receives a signal from its target domain (e.g., the PPG->ECG generator gets a real ECG). It should learn to do nothing."
      ],
      "metadata": {
        "id": "Fs9T2O2uhLui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 5. Loss Functions & Optimizers ===\n",
        "\n",
        "# --- Loss Weights ---\n",
        "# Cycle loss controls reconstruction\n",
        "LAMBDA_CYCLE = 10.0\n",
        "# Identity loss helps preserve color/tone (or in our case, signal characteristics)\n",
        "LAMBDA_IDENTITY = 0.5 * LAMBDA_CYCLE\n",
        "\n",
        "# --- Optimizers ---\n",
        "# We need four separate optimizers\n",
        "generator_g_optimizer = Adam(2e-4, beta_1=0.5) # G: PPG -> ECG\n",
        "generator_f_optimizer = Adam(2e-4, beta_1=0.5) # F: ECG -> PPG\n",
        "discriminator_x_optimizer = Adam(2e-4, beta_1=0.5) # D_X: Distinguishes real/fake PPG\n",
        "discriminator_y_optimizer = Adam(2e-4, beta_1=0.5) # D_Y: Distinguishes real/fake ECG\n",
        "\n",
        "# --- Loss Objects ---\n",
        "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "L1_loss = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "def discriminator_loss(real, fake):\n",
        "    real_loss = loss_obj(tf.ones_like(real), real)\n",
        "    fake_loss = loss_obj(tf.zeros_like(fake), fake)\n",
        "    return (real_loss + fake_loss) * 0.5\n",
        "\n",
        "def generator_loss(generated):\n",
        "    # The generator wants the discriminator to believe its output is real\n",
        "    return loss_obj(tf.ones_like(generated), generated)\n",
        "\n",
        "def calc_cycle_loss(real_image, cycled_image):\n",
        "    loss = L1_loss(real_image, cycled_image)\n",
        "    return LAMBDA_CYCLE * loss\n",
        "\n",
        "def calc_identity_loss(real_image, same_image):\n",
        "    loss = L1_loss(real_image, same_image)\n",
        "    return LAMBDA_IDENTITY * loss"
      ],
      "metadata": {
        "id": "jllaGRjbhMzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block instantiates all four of our models and creates Checkpoint objects to save our progress during training."
      ],
      "metadata": {
        "id": "nbmCXe1whQ5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 6. Build Models & Checkpoints ===\n",
        "\n",
        "if 'train_dataset' in locals():\n",
        "    # --- Instantiate Models ---\n",
        "    # G: PPG -> ECG\n",
        "    generator_g = build_generator(name=\"generator_g\")\n",
        "    # F: ECG -> PPG\n",
        "    generator_f = build_generator(name=\"generator_f\")\n",
        "    # D_X: Discriminator for PPG\n",
        "    discriminator_x = build_discriminator(name=\"discriminator_x\")\n",
        "    # D_Y: Discriminator for ECG\n",
        "    discriminator_y = build_discriminator(name=\"discriminator_y\")\n",
        "\n",
        "    print(\"--- Models Built ---\")\n",
        "\n",
        "    # --- Checkpoint Saver ---\n",
        "    checkpoint_path = \"./checkpoints/train\"\n",
        "    ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
        "                               generator_f=generator_f,\n",
        "                               discriminator_x=discriminator_x,\n",
        "                               discriminator_y=discriminator_y,\n",
        "                               generator_g_optimizer=generator_g_optimizer,\n",
        "                               generator_f_optimizer=generator_f_optimizer,\n",
        "                               discriminator_x_optimizer=discriminator_x_optimizer,\n",
        "                               discriminator_y_optimizer=discriminator_y_optimizer)\n",
        "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "    print(f\"Checkpoints will be saved to {checkpoint_path}\")\n",
        "else:\n",
        "    print(\"Skipping Section 6: train_dataset not found.\")"
      ],
      "metadata": {
        "id": "ec01wayehS2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the most important function. We wrap it in @tf.function to compile it into a high-performance TensorFlow graph.\n",
        "\n",
        "This function performs one full step of training for all four networks:\n",
        "\n",
        "Forward Pass: Generates fake signals, cycled signals, and identity signals.\n",
        "\n",
        "Loss Calculation: Calculates all losses (GAN, cycle, and identity) for both generators.\n",
        "\n",
        "Discriminator Loss: Calculates the losses for both discriminators.\n",
        "\n",
        "Gradients: Calculates the gradients for all four networks based on their respective losses.\n",
        "\n",
        "Apply Gradients: Applies the gradients to update the weights of all four networks using their optimizers."
      ],
      "metadata": {
        "id": "9QVeVW8PhUTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 7. The Custom train_step ===\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_x, real_y):\n",
        "    # real_x is PPG, real_y is ECG\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "        # --- Generator G (PPG -> ECG) ---\n",
        "        fake_y = generator_g(real_x, training=True)\n",
        "        cycled_x = generator_f(fake_y, training=True)\n",
        "\n",
        "        # --- Generator F (ECG -> PPG) ---\n",
        "        fake_x = generator_f(real_y, training=True)\n",
        "        cycled_y = generator_g(fake_x, training=True)\n",
        "\n",
        "        # --- Identity mapping ---\n",
        "        # G should not change an ECG signal\n",
        "        same_y = generator_g(real_y, training=True)\n",
        "        # F should not change a PPG signal\n",
        "        same_x = generator_f(real_x, training=True)\n",
        "\n",
        "        # --- Discriminator decisions ---\n",
        "        disc_real_x = discriminator_x(real_x, training=True) # D_X on real PPG\n",
        "        disc_real_y = discriminator_y(real_y, training=True) # D_Y on real ECG\n",
        "        disc_fake_x = discriminator_x(fake_x, training=True) # D_X on fake PPG\n",
        "        disc_fake_y = discriminator_y(fake_y, training=True) # D_Y on fake ECG\n",
        "\n",
        "        # --- Generator Losses ---\n",
        "        gen_g_loss = generator_loss(disc_fake_y) # G wants D_Y to think fake_y is real\n",
        "        gen_f_loss = generator_loss(disc_fake_x) # F wants D_X to think fake_x is real\n",
        "\n",
        "        # --- Cycle Losses ---\n",
        "        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
        "\n",
        "        # --- Total Generator Loss ---\n",
        "        total_gen_g_loss = gen_g_loss + total_cycle_loss + calc_identity_loss(real_y, same_y)\n",
        "        total_gen_f_loss = gen_f_loss + total_cycle_loss + calc_identity_loss(real_x, same_x)\n",
        "\n",
        "        # --- Discriminator Losses ---\n",
        "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
        "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
        "\n",
        "    # --- Calculate Gradients ---\n",
        "    generator_g_gradients = tape.gradient(total_gen_g_loss, generator_g.trainable_variables)\n",
        "    generator_f_gradients = tape.gradient(total_gen_f_loss, generator_f.trainable_variables)\n",
        "    discriminator_x_gradients = tape.gradient(disc_x_loss, discriminator_x.trainable_variables)\n",
        "    discriminator_y_gradients = tape.gradient(disc_y_loss, discriminator_y.trainable_variables)\n",
        "\n",
        "    # Apply gradients\n",
        "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, generator_g.trainable_variables))\n",
        "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, generator_f.trainable_variables))\n",
        "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients, discriminator_x.trainable_variables))\n",
        "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients, discriminator_y.trainable_variables))\n",
        "\n",
        "    return total_gen_g_loss, total_gen_f_loss, disc_x_loss, disc_y_loss, total_cycle_loss\n",
        "\n",
        "print(\"--- train_step function defined ---\")"
      ],
      "metadata": {
        "id": "c-9EESNBhX6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main loop where we run the training. It iterates for a set number of epochs.\n",
        "\n",
        "Warning: GANs take a long time to train. 20 epochs might take several hours. You may want to start with 1 or 2 epochs to ensure it works.\n",
        "\n",
        "We iterate through our train_dataset, calling train_step for each batch.\n",
        "\n",
        "We print the losses every 100 steps.\n",
        "\n",
        "At the end of each epoch, we save a checkpoint."
      ],
      "metadata": {
        "id": "6ae_w_wDhZ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 8. The Training Loop ===\n",
        "\n",
        "if 'train_dataset' in locals():\n",
        "    print(f\"--- Starting Training for {EPOCHS} epochs ---\")\n",
        "    print(f\"Batch size: {BATCH_SIZE}, Steps per epoch: {len(X_train_ppg) // BATCH_SIZE}\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "        n = 0\n",
        "        for real_ppg, real_ecg in tqdm(train_dataset, desc=\"Epoch Progress\"):\n",
        "            # Run the training step\n",
        "            g_loss, f_loss, dx_loss, dy_loss, cycle_loss = train_step(real_ppg, real_ecg)\n",
        "\n",
        "            if n % 200 == 0:\n",
        "                print(f\"  Step {n}: G_loss={g_loss:.4f}, F_loss={f_loss:.4f}, D_X_loss={dx_loss:.4f}, D_Y_loss={dy_loss:.4f}, Cycle_loss={cycle_loss:.4f}\")\n",
        "            n += 1\n",
        "\n",
        "        # Save checkpoint at the end of the epoch\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "        print(f'Time for epoch {epoch + 1} is {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "    print(\"--- Training Finished ---\")\n",
        "else:\n",
        "    print(\"Skipping Section 8: train_dataset not found.\")"
      ],
      "metadata": {
        "id": "TrhIjMxKhbf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, you can use this block to evaluate your model.\n",
        "\n",
        "We run the generator_g (PPG -> ECG) on the entire test set.\n",
        "\n",
        "We calculate the Mean Absolute Error (MAE) and Mean Squared Error (MSE) between the generated ECG and the real ECG.\n",
        "\n",
        "This gives you the final quantitative metrics for your report.\n",
        "\n",
        "(You can also add code here to plot a few examples using matplotlib.)"
      ],
      "metadata": {
        "id": "K1_H7SvtheIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 9. Evaluation & Inference ===\n",
        "\n",
        "if 'test_dataset' in locals():\n",
        "    print(\"--- Evaluating model on Test Set ---\")\n",
        "\n",
        "    all_maes = []\n",
        "    all_mses = []\n",
        "\n",
        "    # Iterate through the test dataset\n",
        "    for test_ppg, test_ecg in tqdm(test_dataset, desc=\"Evaluating\"):\n",
        "        # Generate a fake ECG from the test PPG\n",
        "        # training=False is important\n",
        "        pred_ecg = generator_g(test_ppg, training=False)\n",
        "\n",
        "        # Calculate errors\n",
        "        mae = L1_loss(test_ecg, pred_ecg)\n",
        "        mse = tf.keras.losses.MeanSquaredError()(test_ecg, pred_ecg)\n",
        "\n",
        "        all_maes.append(mae.numpy())\n",
        "        all_mses.append(mse.numpy())\n",
        "\n",
        "    # --- Report Final Metrics ---\n",
        "    final_mae = np.mean(all_maes)\n",
        "    final_mse = np.mean(all_mses)\n",
        "    final_rmse = np.sqrt(final_mse)\n",
        "\n",
        "    print(\"\\n--- CycleGAN Model Test Results ---\")\n",
        "    print(f\"Test Set MAE:   {final_mae:.4f}\")\n",
        "    print(f\"Test Set MSE:   {final_mse:.4f}\")\n",
        "    print(f\"Test Set RMSE:  {final_rmse:.4f}\")\n",
        "    print(\"-----------------------------------\")\n",
        "\n",
        "    # You can add plotting code here\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # ...\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Section 9: test_dataset not found.\")"
      ],
      "metadata": {
        "id": "9f4Hqlt4hfER"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}