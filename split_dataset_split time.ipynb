{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2d7ac-b6c5-4a20-a8f6-b7542b6087e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "INPUT_DIR   = Path(r\"working file\")   \n",
    "OUTPUT_ROOT = Path(r\"output folder\")  \n",
    "WINDOW_SEC  = 6.0 # length of each small slice\n",
    "RANDOM_SEED = 1234 # random seed for devide into train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2afe53-01d8-44be-a032-d41bd19eb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a53ff-d608-469f-9507-39f492b2383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _detect_time_col(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"t_sec\", \"time\", \"Time\", \"t\", \"seconds\", \"sec\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return df.columns[0]  \n",
    "\n",
    "def _find_first_col_containing(df: pd.DataFrame, needle: str) -> Optional[str]:\n",
    "    needle_lower = needle.lower()\n",
    "    for col in df.columns:\n",
    "        if needle_lower in str(col).lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def _continuous_runs(t: np.ndarray) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    return the time period\n",
    "    \"\"\"\n",
    "    if t.size == 0:\n",
    "        return []\n",
    "    breaks = np.where(np.diff(t) < 0)[0]\n",
    "    starts = [0] + (breaks + 1).tolist()\n",
    "    ends   = (breaks + 1).tolist() + [t.size]\n",
    "    return list(zip(starts, ends))\n",
    "\n",
    "def _tolerance_from_dt(t: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    test the time length\n",
    "    \"\"\"\n",
    "    if t.size < 2:\n",
    "        return 0.01\n",
    "    dt = np.median(np.diff(t))\n",
    "    if not np.isfinite(dt) or dt <= 0:\n",
    "        return 0.01\n",
    "    return float(dt) * 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b0ae5-c413-4c22-90a1-a6c4ae978065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into small part\n",
    "def split_one_csv(csv_path: Path, window_sec: float = WINDOW_SEC):\n",
    "    \"\"\"\n",
    "    return (segments, spans)\n",
    "    - segments: List[pd.DataFrame] as [t_sec, ECG, PPG, ABP]\n",
    "    - spans:    List[(start_time, end_time)]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] read fail {csv_path.name}: {e}\")\n",
    "        return [], []\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"[SKIP] empty file {csv_path.name}\")\n",
    "        return [], []\n",
    "\n",
    "    # col name\n",
    "    time_col = _detect_time_col(df)\n",
    "    ecg_col  = _find_first_col_containing(df, \"ECG\")\n",
    "    ppg_col  = _find_first_col_containing(df, \"PPG\")\n",
    "    abp_col  = _find_first_col_containing(df, \"ABP\")\n",
    "\n",
    "    missing = [n for n, c in [(\"time\", time_col), (\"ECG\", ecg_col), (\"PPG\", ppg_col), (\"ABP\", abp_col)] if c is None]\n",
    "    if missing:\n",
    "        print(f\"[SKIP] {csv_path.name}: no column: {', '.join(missing)}\")\n",
    "        return [], []\n",
    "        \n",
    "    for c in [time_col, ecg_col, ppg_col, abp_col]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[time_col]).sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    t = df[time_col].to_numpy()\n",
    "    runs = _continuous_runs(t)\n",
    "\n",
    "    segments, spans = [], []\n",
    "    for s, e in runs:\n",
    "        run = df.iloc[s:e].copy()\n",
    "        if len(run) < 2:\n",
    "            continue\n",
    "        t_run = run[time_col].to_numpy()\n",
    "        tol   = _tolerance_from_dt(t_run)\n",
    "\n",
    "        t0, t_last = float(t_run[0]), float(t_run[-1])\n",
    "        total_dur  = t_last - t0\n",
    "        if total_dur < window_sec - tol:\n",
    "            # if very short\n",
    "            continue\n",
    "\n",
    "        # total files\n",
    "        n_windows = int(math.floor((t_last - t0) / window_sec))\n",
    "\n",
    "        for k in range(n_windows):\n",
    "            w_start = t0 + k * window_sec\n",
    "            w_end   = w_start + window_sec\n",
    "            mask = (run[time_col] >= w_start) & (run[time_col] < w_end)\n",
    "            seg  = run.loc[mask].copy()\n",
    "            if seg.empty:\n",
    "                continue\n",
    "\n",
    "            # covered time\n",
    "            cov = float(seg[time_col].iloc[-1] - seg[time_col].iloc[0])\n",
    "            if cov < (window_sec - tol):\n",
    "                continue\n",
    "\n",
    "            # delete empty value\n",
    "            if seg[[ecg_col, ppg_col, abp_col]].isna().any().any():\n",
    "                continue\n",
    "\n",
    "            # rename col name\n",
    "            seg = seg[[time_col, ecg_col, ppg_col, abp_col]].rename(\n",
    "                columns={time_col: \"t_sec\", ecg_col: \"ECG\", ppg_col: \"PPG\", abp_col: \"ABP\"}\n",
    "            )\n",
    "            segments.append(seg)\n",
    "            spans.append((w_start, w_end))\n",
    "\n",
    "    return segments, spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808368c-072b-43f4-b521-8a425083686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide train/val/test\n",
    "@dataclass\n",
    "class SegmentInfo:\n",
    "    set_name: str\n",
    "    origin_file: Path\n",
    "    out_path: Path\n",
    "    rows: int\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "\n",
    "def distribute_and_save(all_segments_per_file, out_root: Path, seed: int = RANDOM_SEED) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    random split into train/val/test and record a list\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    sets = [\"train\", \"val\", \"test\"]\n",
    "    manifest_rows = []\n",
    "\n",
    "    for origin, segs, spans in all_segments_per_file:\n",
    "        if not segs:\n",
    "            continue\n",
    "\n",
    "        idxs = list(range(len(segs)))\n",
    "        rng.shuffle(idxs) \n",
    "\n",
    "        for i, seg_idx in enumerate(idxs):\n",
    "            set_name = sets[i % 3]\n",
    "            seg = segs[seg_idx]\n",
    "            st, et = spans[seg_idx]\n",
    "\n",
    "            subdir = out_root / set_name / origin.stem\n",
    "            subdir.mkdir(parents=True, exist_ok=True)\n",
    "            out_path = subdir / f\"{origin.stem}_seg{i:04d}.csv\"\n",
    "            seg.to_csv(out_path, index=False)\n",
    "\n",
    "            manifest_rows.append({\n",
    "                \"set\": set_name,\n",
    "                \"origin_file\": str(origin),\n",
    "                \"out_csv\": str(out_path),\n",
    "                \"rows\": int(len(seg)),\n",
    "                \"start_time\": float(st),\n",
    "                \"end_time\": float(et)\n",
    "            })\n",
    "\n",
    "    man_df = pd.DataFrame(manifest_rows)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "    if not man_df.empty:\n",
    "        man_df.to_csv(out_root / \"manifest.csv\", index=False)\n",
    "    return man_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b250b5-8e62-459d-b5e1-4b220c4de54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_one_csv_iter(csv_path: Path, window_sec: float = WINDOW_SEC):\n",
    "    \"\"\"\n",
    "    generate files\n",
    "    yield: (seg_df, w_start, w_end)，列为[t_sec, ECG, PPG, ABP]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, engine=\"pyarrow\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] read fail {csv_path.name}: {e}\")\n",
    "        return\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"[SKIP] empty file {csv_path.name}\")\n",
    "        return\n",
    "\n",
    "    time_col = _detect_time_col(df)\n",
    "    ecg_col  = _find_first_col_containing(df, \"ECG\")\n",
    "    ppg_col  = _find_first_col_containing(df, \"PPG\")\n",
    "    abp_col  = _find_first_col_containing(df, \"ABP\")\n",
    "\n",
    "    missing = [n for n, c in [(\"time\", time_col), (\"ECG\", ecg_col), (\"PPG\", ppg_col), (\"ABP\", abp_col)] if c is None]\n",
    "    if missing:\n",
    "        print(f\"[SKIP] {csv_path.name}: no column: {', '.join(missing)}\")\n",
    "        return\n",
    "\n",
    "    for c in [time_col, ecg_col, ppg_col, abp_col]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[time_col]).sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    t = df[time_col].to_numpy()\n",
    "    runs = _continuous_runs(t)\n",
    "\n",
    "    for s, e in runs:\n",
    "        run = df.iloc[s:e].copy()\n",
    "        if len(run) < 2:\n",
    "            continue\n",
    "        t_run = run[time_col].to_numpy()\n",
    "        tol   = _tolerance_from_dt(t_run)\n",
    "\n",
    "        t0, t_last = float(t_run[0]), float(t_run[-1])\n",
    "        total_dur  = t_last - t0\n",
    "        if total_dur < window_sec - tol:\n",
    "            continue\n",
    "\n",
    "        n_windows = int(math.floor((t_last - t0) / window_sec))\n",
    "        for k in range(n_windows):\n",
    "            w_start = t0 + k * window_sec\n",
    "            w_end   = w_start + window_sec\n",
    "            mask = (run[time_col] >= w_start) & (run[time_col] < w_end)\n",
    "            seg  = run.loc[mask].copy()\n",
    "            if seg.empty:\n",
    "                continue\n",
    "\n",
    "            cov = float(seg[time_col].iloc[-1] - seg[time_col].iloc[0])\n",
    "            if cov < (window_sec - tol):\n",
    "                continue\n",
    "\n",
    "            if seg[[ecg_col, ppg_col, abp_col]].isna().any().any():\n",
    "                continue\n",
    "\n",
    "            seg = seg[[time_col, ecg_col, ppg_col, abp_col]].rename(\n",
    "                columns={time_col: \"t_sec\", ecg_col: \"ECG\", ppg_col: \"PPG\", abp_col: \"ABP\"}\n",
    "            )\n",
    "            yield seg, w_start, w_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919dd87a-aab1-4e18-a8b8-c1713dfb40dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multi threads\n",
    "import os, csv, threading, random, hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "MAX_WORKERS = 24   # total threads\n",
    "sets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "csv_files = sorted([p for p in INPUT_DIR.glob(\"**/*.csv\") if p.is_file()])\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"在 {INPUT_DIR} 下没有找到 CSV 文件\")\n",
    "\n",
    "# 初始化 manifest：写表头（如不存在）\n",
    "manifest_path = OUTPUT_ROOT / \"manifest.csv\"\n",
    "if not manifest_path.exists():\n",
    "    with open(manifest_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"set\", \"origin_file\", \"out_csv\", \"rows\", \"start_time\", \"end_time\"])\n",
    "        writer.writeheader()\n",
    "\n",
    "# 共享的manifest写入器与锁（同一文件句柄，线程间共享，写入时加锁）\n",
    "manifest_lock = threading.Lock()\n",
    "_manifest_file = open(manifest_path, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "_manifest_writer = csv.DictWriter(_manifest_file, fieldnames=[\"set\", \"origin_file\", \"out_csv\", \"rows\", \"start_time\", \"end_time\"])\n",
    "\n",
    "# 全局统计（上锁更新）\n",
    "total_written = 0\n",
    "set_counts = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "\n",
    "def process_one_file(csv_path: Path):\n",
    "    \"\"\"\n",
    "    线程任务：处理单个CSV，逐片段切分->分配->写盘->记录manifest。\n",
    "    返回：(written_this_file, local_counts_dict)\n",
    "    \"\"\"\n",
    "    for sn in (\"train\", \"val\", \"test\"):\n",
    "        (OUTPUT_ROOT / sn / csv_path.stem).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    global total_written  # 这里声明使用全局的 total_written\n",
    "    import pandas as pd\n",
    "    import hashlib, random\n",
    "\n",
    "    per_file_idx = 0\n",
    "    written_this_file = 0\n",
    "    local_counts = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "\n",
    "    # 为该文件创建确定性的随机序列（保证可复现）\n",
    "    seed_int = int(hashlib.md5(str(csv_path).encode(\"utf-8\")).hexdigest()[:8], 16) ^ RANDOM_SEED\n",
    "    rng = random.Random(seed_int)\n",
    "    order = [0, 1, 2]\n",
    "    rng.shuffle(order)\n",
    "\n",
    "    try:\n",
    "        for seg, st, et in split_one_csv_iter(csv_path, window_sec=WINDOW_SEC):\n",
    "            set_name = sets[order[per_file_idx % 3]]\n",
    "            if per_file_idx % 3 == 2:\n",
    "                rng.shuffle(order)\n",
    "\n",
    "            subdir = OUTPUT_ROOT / set_name / csv_path.stem\n",
    "            #subdir.mkdir(parents=True, exist_ok=True)\n",
    "            out_path = subdir / f\"{csv_path.stem}_seg{per_file_idx:04d}.csv\"\n",
    "\n",
    "            seg.to_csv(out_path, index=False)\n",
    "\n",
    "            # ★ 用锁保护：写 manifest + 更新计数\n",
    "            with manifest_lock:\n",
    "                _manifest_writer.writerow({\n",
    "                    \"set\": set_name,\n",
    "                    \"origin_file\": str(csv_path),\n",
    "                    \"out_csv\": str(out_path),\n",
    "                    \"rows\": int(len(seg)),\n",
    "                    \"start_time\": float(st),\n",
    "                    \"end_time\": float(et),\n",
    "                })\n",
    "                #_manifest_file.flush()\n",
    "\n",
    "                total_written += 1            # 这里已是全局变量\n",
    "                set_counts[set_name] += 1     # 字典项就地修改，无需 global\n",
    "\n",
    "            per_file_idx += 1\n",
    "            written_this_file += 1\n",
    "            local_counts[set_name] += 1\n",
    "\n",
    "        print(f\"{csv_path.name}: 写出有效 6s 片段 {written_this_file} 个\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 处理 {csv_path.name} 出错: {e}\")\n",
    "\n",
    "    with manifest_lock:\n",
    "        _manifest_file.flush()\n",
    "\n",
    "    return written_this_file, local_counts\n",
    "\n",
    "\n",
    "# 在线程池中并行处理多个CSV\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = [ex.submit(process_one_file, p) for p in csv_files]\n",
    "    # 等待全部完成（异常会在 result() 时抛出）\n",
    "    for fut in as_completed(futures):\n",
    "        _ = fut.result()\n",
    "\n",
    "# 关闭manifest文件\n",
    "_manifest_file.close()\n",
    "\n",
    "# 汇总\n",
    "if total_written == 0:\n",
    "    print(\"没有产出有效片段：请检查列名(含ECG/PPG/ABP)、时间列，以及数据是否存在空值。\")\n",
    "else:\n",
    "    print(f\"\\n完成：共写出 {total_written} 个 6s 片段，线程数={MAX_WORKERS}\")\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        print(f\"  {k:>5}: {set_counts.get(k, 0)}\")\n",
    "    print(f\"\\n清单文件：{manifest_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc31ad-698f-4d10-bde5-4908d8ba96d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_11785",
   "language": "python",
   "name": "project_11785"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
