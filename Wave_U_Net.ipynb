{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This first code block will install the necessary xlrd library for reading older Excel files (though your data is .csv, this was in your original code) and import all required libraries from TensorFlow, Keras, Pandas, and Numpy."
      ],
      "metadata": {
        "id": "umoZuhBcd4dG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN7FHmbcdxmc"
      },
      "outputs": [],
      "source": [
        "# === 1. Install and Import Libraries ===\n",
        "\n",
        "# Install xlrd if not already present\n",
        "!pip install -q xlrd\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "import shutil\n",
        "\n",
        "# Sklearn for metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# TensorFlow/Keras for Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, LeakyReLU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section handles all data preparation. It includes:\n",
        "\n",
        "Mounting your Google Drive.\n",
        "\n",
        "unzip_data: A function to extract your .zip files.\n",
        "\n",
        "create_sequences_ppg_to_ecg: The most important function. It reads a dataframe, normalizes the PPG and ECG signals, and slices them into overlapping windows.\n",
        "\n",
        "Input (X): A window of the PPG signal.\n",
        "\n",
        "Output (y): The corresponding window of the ECG signal.\n",
        "\n",
        "load_and_process: The main wrapper function that orchestrates the unzipping and sequence creation for all your data splits."
      ],
      "metadata": {
        "id": "pZWbUlFXd6on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 2. Mount Drive & Define Data Functions ===\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def unzip_data(zip_path, extract_folder):\n",
        "    \"\"\"Unzips a file and returns a list of all .csv files inside.\"\"\"\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Error: {zip_path} not found. Check your Google Drive path.\")\n",
        "        return []\n",
        "    os.makedirs(extract_folder, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_folder)\n",
        "    csv_files = glob.glob(os.path.join(extract_folder, '**/*.csv'), recursive=True)\n",
        "    print(f\"Extracted {len(csv_files)} files from {zip_path}\")\n",
        "    return csv_files\n",
        "\n",
        "def create_sequences_ppg_to_ecg(df, seq_length=256, step=128):\n",
        "    \"\"\"\n",
        "    Creates overlapping sequences for PPG-to-ECG translation.\n",
        "    Input (X) is PPG signal.\n",
        "    Output (y) is ECG signal.\n",
        "    \"\"\"\n",
        "    ecg = df['ECG'].values\n",
        "    ppg = df['PPG'].values\n",
        "\n",
        "    # Normalize signals individually\n",
        "    ecg = (ecg - np.mean(ecg)) / (np.std(ecg) + 1e-6)\n",
        "    ppg = (ppg - np.mean(ppg)) / (np.std(ppg) + 1e-6)\n",
        "\n",
        "    X_seq = []\n",
        "    y_seq = []\n",
        "\n",
        "    for i in range(0, len(df) - seq_length, step):\n",
        "        end_idx = i + seq_length\n",
        "\n",
        "        X_window = ppg[i:end_idx]\n",
        "        y_window = ecg[i:end_idx]\n",
        "\n",
        "        if np.std(X_window) > 0.1 and np.std(y_window) > 0.1:\n",
        "            X_seq.append(X_window)\n",
        "            y_seq.append(y_window)\n",
        "\n",
        "    # Add a \"channels\" dimension\n",
        "    return np.expand_dims(np.array(X_seq), -1), np.expand_dims(np.array(y_seq), -1)\n",
        "\n",
        "def load_and_process(zip_path, extract_folder, seq_length=256, debug_limit=None):\n",
        "    \"\"\"Main function to load zips and process all files for sequence models.\"\"\"\n",
        "    file_list = unzip_data(zip_path, extract_folder)\n",
        "    if debug_limit is not None:\n",
        "        file_list = file_list[:debug_limit]\n",
        "        print(f\"--- DEBUG MODE: Processing only {len(file_list)} files. ---\")\n",
        "\n",
        "    if not file_list: return np.array([]), np.array([])\n",
        "    all_X, all_y = [], []\n",
        "\n",
        "    for f in tqdm(file_list, desc=f\"Processing {zip_path}\"):\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read {f}: {e}\")\n",
        "            continue\n",
        "        if not all(col in df.columns for col in ['t_sec', 'ECG', 'PPG', 'ABP']):\n",
        "            print(f\"Skipping {f}: missing required columns.\")\n",
        "            continue\n",
        "\n",
        "        X, y = create_sequences_ppg_to_ecg(df, seq_length=seq_length)\n",
        "        if X.shape[0] > 0:\n",
        "            all_X.append(X)\n",
        "            all_y.append(y)\n",
        "\n",
        "    if not all_X:\n",
        "        print(f\"No valid data found in {zip_path} for sequence mode.\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    all_X = np.concatenate(all_X, axis=0)\n",
        "    all_y = np.concatenate(all_y, axis=0)\n",
        "    print(f\"Finished processing {zip_path}. Found {all_X.shape[0]} samples.\")\n",
        "    return all_X, all_y"
      ],
      "metadata": {
        "id": "hZFCFVA2d_PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key features of this architecture:\n",
        "\n",
        "Encoder: Uses Conv1D layers with strides=2 to downsample the signal. This is a learnable downsampling, which is more powerful than a fixed MaxPooling operation.\n",
        "\n",
        "Decoder: Uses Conv1DTranspose with strides=2 to upsample the signal back to its original length.\n",
        "\n",
        "Skip Connections: Concatenate layers merge the high-resolution feature maps from the encoder with the upsampled maps from the decoder. This is crucial for preserving the exact timing and sharp details (like the QRS complex) from the original signal.\n",
        "\n",
        "Activation: We use LeakyReLU as recommended by the Wave-U-Net paper, which can help prevent \"dying ReLU\" problems."
      ],
      "metadata": {
        "id": "GN3SW6kbeCr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 3. Wave-U-Net Model Definition ===\n",
        "\n",
        "def build_wave_unet_model(input_shape=(256, 1), kernel_size=15, filters=16):\n",
        "    \"\"\"\n",
        "    Builds a 1D Wave-U-Net model.\n",
        "    Downsampling is done with Conv1D(strides=2).\n",
        "    Upsampling is done with Conv1DTranspose(strides=2).\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # --- Encoder Path ---\n",
        "    # Level 1\n",
        "    c1 = Conv1D(filters, kernel_size, padding='same')(inputs)\n",
        "    c1 = LeakyReLU(0.2)(c1)\n",
        "    c1 = Conv1D(filters, kernel_size, padding='same')(c1)\n",
        "    c1 = LeakyReLU(0.2)(c1)\n",
        "    d1 = Conv1D(filters, kernel_size, strides=2, padding='same')(c1) # Downsample\n",
        "    d1 = LeakyReLU(0.2)(d1)\n",
        "\n",
        "    # Level 2\n",
        "    c2 = Conv1D(filters*2, kernel_size, padding='same')(d1)\n",
        "    c2 = LeakyReLU(0.2)(c2)\n",
        "    c2 = Conv1D(filters*2, kernel_size, padding='same')(c2)\n",
        "    c2 = LeakyReLU(0.2)(c2)\n",
        "    d2 = Conv1D(filters*2, kernel_size, strides=2, padding='same')(c2) # Downsample\n",
        "    d2 = LeakyReLU(0.2)(d2)\n",
        "\n",
        "    # Level 3\n",
        "    c3 = Conv1D(filters*4, kernel_size, padding='same')(d2)\n",
        "    c3 = LeakyReLU(0.2)(c3)\n",
        "    c3 = Conv1D(filters*4, kernel_size, padding='same')(c3)\n",
        "    c3 = LeakyReLU(0.2)(c3)\n",
        "    d3 = Conv1D(filters*4, kernel_size, strides=2, padding='same')(c3) # Downsample\n",
        "    d3 = LeakyReLU(0.2)(d3)\n",
        "\n",
        "    # Level 4\n",
        "    c4 = Conv1D(filters*8, kernel_size, padding='same')(d3)\n",
        "    c4 = LeakyReLU(0.2)(c4)\n",
        "    c4 = Conv1D(filters*8, kernel_size, padding='same')(c4)\n",
        "    c4 = LeakyReLU(0.2)(c4)\n",
        "    d4 = Conv1D(filters*8, kernel_size, strides=2, padding='same')(c4) # Downsample\n",
        "    d4 = LeakyReLU(0.2)(d4)\n",
        "\n",
        "    # --- Bottleneck ---\n",
        "    b = Conv1D(filters*16, kernel_size, padding='same')(d4)\n",
        "    b = LeakyReLU(0.2)(b)\n",
        "    b = Conv1D(filters*16, kernel_size, padding='same')(b)\n",
        "    b = LeakyReLU(0.2)(b)\n",
        "\n",
        "    # --- Decoder Path ---\n",
        "    # Level 4\n",
        "    u4 = Conv1DTranspose(filters*8, kernel_size, strides=2, padding='same')(b)\n",
        "    u4 = LeakyReLU(0.2)(u4)\n",
        "    u4 = Concatenate()([u4, c4]) # Skip connection\n",
        "    u4_conv = Conv1D(filters*8, kernel_size, padding='same')(u4)\n",
        "    u4_conv = LeakyReLU(0.2)(u4_conv)\n",
        "    u4_conv = Conv1D(filters*8, kernel_size, padding='same')(u4_conv)\n",
        "    u4_conv = LeakyReLU(0.2)(u4_conv)\n",
        "\n",
        "    # Level 3\n",
        "    u3 = Conv1DTranspose(filters*4, kernel_size, strides=2, padding='same')(u4_conv)\n",
        "    u3 = LeakyReLU(0.2)(u3)\n",
        "    u3 = Concatenate()([u3, c3]) # Skip connection\n",
        "    u3_conv = Conv1D(filters*4, kernel_size, padding='same')(u3)\n",
        "    u3_conv = LeakyReLU(0.2)(u3_conv)\n",
        "    u3_conv = Conv1D(filters*4, kernel_size, padding='same')(u3_conv)\n",
        "    u3_conv = LeakyReLU(0.2)(u3_conv)\n",
        "\n",
        "    # Level 2\n",
        "    u2 = Conv1DTranspose(filters*2, kernel_size, strides=2, padding='same')(u3_conv)\n",
        "    u2 = LeakyReLU(0.2)(u2)\n",
        "    u2 = Concatenate()([u2, c2]) # Skip connection\n",
        "    u2_conv = Conv1D(filters*2, kernel_size, padding='same')(u2)\n",
        "    u2_conv = LeakyReLU(0.2)(u2_conv)\n",
        "    u2_conv = Conv1D(filters*2, kernel_size, padding='same')(u2_conv)\n",
        "    u2_conv = LeakyReLU(0.2)(u2_conv)\n",
        "\n",
        "    # Level 1\n",
        "    u1 = Conv1DTranspose(filters, kernel_size, strides=2, padding='same')(u2_conv)\n",
        "    u1 = LeakyReLU(0.2)(u1)\n",
        "    u1 = Concatenate()([u1, c1]) # Skip connection\n",
        "    u1_conv = Conv1D(filters, kernel_size, padding='same')(u1)\n",
        "    u1_conv = LeakyReLU(0.2)(u1_conv)\n",
        "    u1_conv = Conv1D(filters, kernel_size, padding='same')(u1_conv)\n",
        "    u1_conv = LeakyReLU(0.2)(u1_conv)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv1D(1, 1, activation='linear')(u1_conv) # 'linear' for regression\n",
        "\n",
        "    return Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "Va6XkXJufgVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define all the \"hyperparameters\" for our model and job.\n",
        "\n",
        "SEQ_LENGTH: 256 is a good power-of-2, which works well with the 4 downsampling steps in the U-Net (256 -> 128 -> 64 -> 32 -> 16).\n",
        "\n",
        "KERNEL_SIZE: A larger kernel (like 15) is common in Wave-U-Nets to capture a wider time-span in each convolution.\n",
        "\n",
        "Paths: You must edit these paths to point to the correct location of your .zip files in Google Drive.\n",
        "\n",
        "After setting the config, we call load_and_process to load all three datasets (train, validation, and test) into memory."
      ],
      "metadata": {
        "id": "N7DKU19FfiL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 4. Configuration & Data Loading ===\n",
        "\n",
        "print(\"\\n--- Starting Wave-U-Net PPG-to-ECG Model ---\")\n",
        "\n",
        "# 1. Define Model Parameters\n",
        "SEQ_LENGTH = 256\n",
        "STEP = 128\n",
        "NUM_FEATURES = 1  # Input is just PPG\n",
        "NUM_OUTPUTS = 1   # Output is just ECG\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "\n",
        "# --- Wave-U-Net-specific Hyperparameters ---\n",
        "KERNEL_SIZE = 15  # Larger kernel size is common in Wave-U-Nets\n",
        "FILTERS = 16      # Starting number of filters\n",
        "# ---------------------------------------------\n",
        "\n",
        "# --- Define Paths ---\n",
        "# !!! EDIT THESE PATHS !!!\n",
        "train_zip_path = '/content/drive/MyDrive/11785FinalData/train.zip'\n",
        "val_zip_path = '/content/drive/MyDrive/11785FinalData/val.zip'\n",
        "test_zip_path = '/content/drive/MyDrive/11785FinalData/test.zip'\n",
        "\n",
        "# 2. Load and process data\n",
        "X_train_seq, y_train_seq = load_and_process(train_zip_path, 'data/train', seq_length=SEQ_LENGTH)\n",
        "X_val_seq, y_val_seq = load_and_process(val_zip_path, 'data/val', seq_length=SEQ_LENGTH)\n",
        "X_test_seq, y_test_seq = load_and_process(test_zip_path, 'data/test', seq_length=SEQ_LENGTH)\n",
        "\n",
        "# Check if data loading was successful\n",
        "if X_train_seq.shape[0] > 0:\n",
        "    print(f\"--- Data Loaded Successfully ---\")\n",
        "    print(f\"Training data shape: {X_train_seq.shape}\")\n",
        "    print(f\"Training labels shape: {y_train_seq.shape}\")\n",
        "    print(f\"Validation data shape: {X_val_seq.shape}\")\n",
        "    print(f\"Test data shape: {X_test_seq.shape}\")\n",
        "else:\n",
        "    print(\"--- ERROR: No training data was loaded. Check your paths and data. ---\")"
      ],
      "metadata": {
        "id": "lfrkmR_kfk6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the final step. We:\n",
        "\n",
        "Wrap the main logic in an if statement to ensure the script only runs if data was loaded correctly.\n",
        "\n",
        "Build the model using our function from Section 3.\n",
        "\n",
        "Compile the model. We use mean_squared_error as the loss function, which is standard for signal regression. We also monitor mean_absolute_error as a more interpretable metric.\n",
        "\n",
        "Print model.summary() so you can see the architecture and parameter count.\n",
        "\n",
        "Train the model using model.fit(), passing in our training and validation data. We use EarlyStopping to prevent overfitting.\n",
        "\n",
        "Evaluate the final model on the unseen test set and print the results."
      ],
      "metadata": {
        "id": "VSd4knGwfmas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 5. Build, Train, and Evaluate ===\n",
        "\n",
        "if X_train_seq.shape[0] > 0:\n",
        "    # 3. Build and compile the Wave-U-Net model\n",
        "    input_shape = (SEQ_LENGTH, NUM_FEATURES)\n",
        "\n",
        "    model = build_wave_unet_model(\n",
        "        input_shape,\n",
        "        kernel_size=KERNEL_SIZE,\n",
        "        filters=FILTERS\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_absolute_error'])\n",
        "\n",
        "    print(\"\\n--- Model Summary ---\")\n",
        "    model.summary()\n",
        "\n",
        "    # 4. Train Model\n",
        "    print(\"\\n--- Training Wave-U-Net model... ---\")\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 5. Evaluate on Test Set\n",
        "    print(\"\\n--- Evaluating Wave-U-Net on test set... ---\")\n",
        "    results = model.evaluate(X_test_seq, y_test_seq, batch_size=BATCH_SIZE)\n",
        "    test_loss = results[0]\n",
        "    test_mae = results[1]\n",
        "\n",
        "    # 6. Report Results\n",
        "    print(\"\\n--- Wave-U-Net Model Test Results ---\")\n",
        "    print(f\"Test Set MSE (Loss): {test_loss:.4f}\")\n",
        "    print(f\"Test Set MAE:        {test_mae:.4f}\")\n",
        "    print(\"-------------------------------------\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping model training due to data loading error.\")"
      ],
      "metadata": {
        "id": "LVsAbnSIfpVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final step, this block saves your trained model (weights and architecture) to your Google Drive. This allows you to reload it later for inference or further training without having to start from scratch."
      ],
      "metadata": {
        "id": "xF6I643Vfr_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 6. Save a Trained Model to Your Drive ===\n",
        "\n",
        "# First, create a path to a folder in your Google Drive\n",
        "save_folder = '/content/drive/My Drive/MyProject'\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "# Define the full path to save your model file\n",
        "model_save_path = os.path.join(save_folder, 'wave_unet_ppg_to_ecg_model.keras')\n",
        "\n",
        "# Save the model\n",
        "try:\n",
        "    model.save(model_save_path)\n",
        "    print(f\"Model successfully saved to: {model_save_path}\")\n",
        "except NameError:\n",
        "    print(\"Could not save model. 'model' variable is not defined.\")\n",
        "    print(\"This likely means the training step was skipped due to a data error.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving: {e}\")"
      ],
      "metadata": {
        "id": "tTp0OKZZfsvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}